\documentclass{book}
\usepackage{amsmath}
\usepackage[letterpaper,left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{tocloft} % For customizing TOC
\usepackage[bookmarks=true,hidelinks]{hyperref}

% Customize Table of Contents indentation
\setlength{\cftchapindent}{0em}    % Chapter indent
\setlength{\cftsecindent}{1.5em}   % Section indent under chapter
\setlength{\cftsubsecindent}{3.5em} % Subsection indent
\renewcommand{\cftchapfont}{\normalfont\bfseries} % Chapter font in TOC
\renewcommand{\cftsecfont}{\normalfont}           % Section font in TOC
\renewcommand{\cftsubsecfont}{\normalfont}        % Subsection font in TOC

% Remove page numbers from empty pages
\usepackage{emptypage}

\title{A General Overview of AI}
\author{Clay Morton}
\date{4/28/25}

\begin{document}

\maketitle

% Force TOC to start immediately after title
\begingroup
\let\cleardoublepage\relax
\tableofcontents
\endgroup

\chapter{Tiers of AI}

\section{Introduction to the Hierarchy of Intelligence Systems}

Artificial Intelligence (AI) is often treated as a single, unified technology in
popular discussions, evoking images of sentient machines and omniscient digital 
assistants. Yet within technical disciplines, AI is understood not as a single 
technology but as a broad field encompassing several distinct, layered domains. 
At the highest level, AI is concerned with replicating various aspects of human 
intelligence. Beneath it lies Machine Learning (ML), which refines this goal by 
enabling machines to learn from data rather than relying exclusively on 
pre-programmed rules. Within Machine Learning, Deep Learning (DL) further 
narrows the approach to architectures composed of layered neural networks that 
can learn intricate patterns autonomously. Neural Networks themselves, 
originally inspired by biological neurons, form the foundational computational 
model enabling much of the current success in AI.

Understanding these hierarchical relationships is crucial because it clarifies 
why various AI systems differ dramatically in complexity, scope, and capability.
It also explains why not every AI system involves Machine Learning, why not 
every Machine Learning application involves Deep Learning, and why discussions 
of AI can sometimes appear confusing or contradictory. In this chapter, we will 
untangle these relationships carefully, building a clear and coherent view of 
the technological landscape.

\section{Artificial Intelligence (AI): The Broad Vision}

The term Artificial Intelligence encompasses the quest to create machines 
capable of performing tasks that, when executed by humans, require intelligence.
This broad goal includes capacities such as learning, reasoning, 
problem-solving, perception, and language comprehension. AI, as a field, dates 
back to the mid-20th century, with early ambitions famously articulated during 
the 1956 Dartmouth Conference, where pioneers imagined that a machine could one 
day replicate every aspect of human intelligence.

A key distinction in AI research is between different levels of cognitive 
capability. Artificial Narrow Intelligence (ANI) refers to systems designed for 
a single, specific task or a restricted set of tasks. For example, a 
recommendation engine that suggests products based on user behavior operates 
within a narrow domain and would be utterly incapable of performing unrelated 
tasks such as language translation. Despite remarkable successes, all current AI
applications—from autonomous vehicles to medical diagnostic tools—fall within 
the narrow AI category.

In contrast, Artificial General Intelligence (AGI) represents the 
still-theoretical goal of creating machines with generalized cognitive 
abilities. An AGI system would not be limited to specific tasks but would 
instead possess the flexibility to transfer learning across domains, adapt to 
unfamiliar situations, and reason abstractly in the way that humans can. No AGI 
system exists today, though it remains a central focus of speculative and 
theoretical research.

Further extending the spectrum is the notion of Artificial Superintelligence 
(ASI), which imagines AI systems that surpass human intelligence across all 
fields, including creativity, emotional understanding, and social acumen. While 
ASI is even more speculative, its potential raises profound questions about the 
future of humanity, governance, and ethics in an AI-driven world.

Thus, while AI as a concept encompasses grand aspirations, the current 
technological reality is firmly rooted in specialized, domain-specific systems 
characterized by narrow expertise and bounded capability.

\section{Machine Learning (ML): Data-Driven Intelligence}

As researchers pursued the dream of intelligent machines, they quickly 
encountered the impracticality of manually programming every conceivable 
scenario a machine might face. This realization led to the emergence of Machine 
Learning, a paradigm shift that focuses on enabling machines to infer patterns 
and make decisions based on data rather than on hardcoded instructions.

In a Machine Learning system, instead of specifying explicit rules for a task, 
we provide the machine with a dataset containing examples. From this data, the 
machine learns an approximate mapping between inputs and desired outputs. 
Consider the task of email spam detection: manually enumerating every possible 
indicator of spam would be impossible, but a Machine Learning model trained on 
thousands of labeled emails can infer subtle statistical patterns, such as word 
usage or sender reputation, to generalize to new, unseen emails.

Machine Learning is commonly divided into three major categories, based on the 
nature of the learning task:

\begin{itemize}
    \item \textbf{Supervised Learning} involves learning a function from labeled
    examples, where each input is associated with a known output. Tasks such as 
    image classification, medical diagnosis, and language translation often fall
    into this category. Here, the model's success depends heavily on the quality
    and representativeness of the labeled training data.
    
    \item \textbf{Unsupervised Learning}, by contrast, deals with unlabeled 
    data. The machine seeks to discover hidden structures, patterns, or 
    groupings within the data itself. Applications such as customer 
    segmentation, anomaly detection, and topic modeling exemplify unsupervised 
    learning techniques, where no external guidance about the 'correct' outcomes
    is available.
    
    \item \textbf{Reinforcement Learning} occupies a somewhat different niche. 
    In this setting, the machine learns by interacting with an environment, 
    receiving feedback in the form of rewards or penalties. Reinforcement 
    Learning underpins many recent breakthroughs in game-playing AI, such as 
    DeepMind's AlphaGo, where the agent learns optimal strategies through 
    trial-and-error experience over millions of simulated games.
\end{itemize}

While each learning paradigm has distinct methodologies and challenges, they 
share a common emphasis on empirical learning from data rather than reliance on 
fixed programming.

\section{Deep Learning (DL): Layered Representations}

As Machine Learning matured, researchers sought ways to automatically extract 
and represent increasingly complex features from raw data, leading to the rise 
of Deep Learning. Deep Learning involves models known as deep neural networks, 
which consist of multiple layers of processing units (neurons) organized in a 
hierarchy. Each successive layer transforms its input into a more abstract and 
composite representation.

One of the key advantages of Deep Learning is its ability to perform 
representation learning. In traditional Machine Learning, much effort is spent 
on feature engineering—manually designing the features that the model will use. 
Deep Learning, by contrast, enables models to learn relevant features 
automatically. For example, in computer vision, early layers of a deep network 
might learn to detect simple patterns such as edges or textures, while deeper 
layers recognize more complex concepts like eyes, faces, or entire objects.

However, this power comes at a cost. Deep Learning models typically require 
large labeled datasets to achieve high accuracy, and their training demands 
substantial computational resources, often relying on specialized hardware 
accelerators such as GPUs or TPUs. Moreover, because of their many layers and 
millions of parameters, deep networks can be opaque, leading to challenges in 
interpretability and debugging.

Despite these challenges, Deep Learning has driven many of the most spectacular 
advances in AI over the past decade. Systems capable of defeating world 
champions in complex games, generating coherent text, translating languages with
near-human accuracy, and driving cars autonomously all rely fundamentally on 
deep architectures.

\section{Neural Networks (NNs): The Computational Core}

At the heart of Deep Learning lie Artificial Neural Networks (ANNs), 
mathematical structures loosely inspired by the organization of neurons in 
biological brains. Each artificial neuron receives one or more inputs, computes 
a weighted sum of these inputs, adds a bias term, and then applies a non-linear 
transformation known as an activation function.

Neural Networks are organized into layers:

\begin{itemize}
    \item The input layer receives the raw data.
    \item One or more hidden layers perform intermediate computations and 
    extract hierarchical features.
    \item The output layer produces the final prediction or decision.
\end{itemize}

The term "deep" in Deep Learning simply refers to networks with many hidden 
layers. Each additional layer allows the model to capture increasingly abstract 
patterns, but also makes training more challenging due to issues such as 
vanishing gradients and overfitting.

A crucial component enabling the expressive power of Neural Networks is the 
activation function. Without activation functions, a network composed of 
multiple layers would collapse into an equivalent single-layer model, 
restricting it to only linear transformations. Common activation functions 
include:

\begin{itemize}
    \item \textbf{ReLU (Rectified Linear Unit)}, which outputs zero for 
    negative inputs and the input itself for positive inputs, enabling efficient 
    and sparse representations.
    \item \textbf{Sigmoid}, which squashes inputs to the range (0,1), 
    historically popular for binary classification tasks but prone to saturation 
    issues.
    \item \textbf{Tanh}, which maps inputs to (-1,1), offering zero-centered 
    activations beneficial for certain architectures.
\end{itemize}

Training a neural network involves optimizing the weights and biases across all 
layers to minimize a loss function, typically through gradient-based 
optimization methods such as stochastic gradient descent (SGD). This process, 
known as backpropagation, systematically updates parameters to reduce prediction
errors.

\section{Visualizing the Hierarchy: AI to Neural Networks}

To better conceptualize the relationships among the fields we have discussed, 
it is helpful to visualize them hierarchically:

\begin{verbatim}
Artificial Intelligence
    -> Machine Learning
        -> Deep Learning
            -> Generative AG
                -> Large Language Models
\end{verbatim}

This structure illustrates the successive specialization at each layer. 
While all Deep Learning is Machine Learning, not all Machine Learning involves 
Deep Learning. Likewise, while all Machine Learning falls under the umbrella of 
AI, many AI systems (especially symbolic AI, expert systems, and rule-based 
reasoning engines) operate without using Machine Learning techniques at all.

Understanding this nested structure clarifies why discussions about "AI" can 
sometimes be misleadingly broad: a news article about "AI" might, in fact, be 
describing a narrow Deep Learning model trained for a single task.

\section{Important Concept: Narrow AI vs. General AI}

The distinction between Narrow AI and General AI is not merely academic; it 
profoundly shapes what current AI systems can and cannot do. Narrow AI systems 
operate within well-defined boundaries. They excel at specific tasks but cannot 
reason outside their training domains. A language model that writes poetry 
cannot drive a car; a medical diagnostic system cannot negotiate business 
contracts.

General AI, on the other hand, would require the ability to learn, reason, and 
adapt across domains without being explicitly retrained. Achieving AGI would 
involve breakthroughs not only in algorithmic design but also in our fundamental 
understanding of learning, abstraction, reasoning, and consciousness.

At present, even the most sophisticated AI models exhibit significant 
limitations, such as brittleness in novel situations, lack of true 
understanding, and vulnerability to adversarial manipulation. Recognizing these 
limitations is critical to tempering expectations and responsibly guiding 
future development.

\section{Chapter Summary}

In this chapter, we explored the layered organization of Artificial Intelligence
technologies. Beginning with the broad ambition of AI to replicate human 
cognitive functions, we examined how Machine Learning narrows this focus by 
enabling machines to learn from data. We then traced how Deep Learning builds 
upon Machine Learning through the use of layered neural networks capable of 
autonomously extracting complex representations. Finally, we examined how 
Neural Networks provide the mathematical and computational substrate for Deep 
Learning's success.

Understanding these distinctions provides clarity not only in interpreting 
modern AI achievements but also in assessing their true capabilities and 
limitations. As we continue our exploration of AI systems, keeping this 
hierarchical structure in mind will be essential to making sense of a 
rapidly evolving technological landscape.

\chapter{Model Architectures and Training}

\section{Introduction: The Architecture of Intelligence}

If the ambition of Artificial Intelligence is to create machines capable of 
perception, reasoning, and autonomous action, then the architectures of AI 
systems form the skeletons upon which that intelligence is built. The 
architecture of a system defines how information flows, how representations are 
formed, how learning occurs, and ultimately how performance is achieved. It 
determines not only the types of tasks the model can handle, but also its 
efficiency, scalability, and flexibility.

In modern AI — especially within Machine Learning and Deep Learning — the 
architecture of a model is not a peripheral detail but a central determinant of 
success. Different architectures embody different assumptions about the 
structure of the data and the nature of the learning problem. Thus, the choice 
and design of a model architecture is deeply tied to the model's strengths, 
weaknesses, and areas of applicability.

In this chapter, we will explore several major classes of neural network 
architectures, tracing their theoretical underpinnings, structural designs, 
advantages, and limitations. We will also examine the training techniques by 
which these models are optimized, uncovering the principles that allow neural 
networks to move from random initializations to effective problem solvers.

\section{Feedforward Neural Networks (FFNNs)}

\subsection{Concept and Structure}

At the heart of most Deep Learning models lies the Feedforward Neural Network 
(FFNN), sometimes referred to as a Multilayer Perceptron (MLP). FFNNs represent 
the simplest and most foundational type of artificial neural network. Despite 
their simplicity, they are powerful and serve as the starting point for 
understanding more complex architectures.

An FFNN is characterized by a unidirectional flow of information. Data moves 
strictly forward through the network — from the input layer through one or more 
hidden layers to the output layer — with no cycles or feedback connections. Each 
neuron in a given layer connects to every neuron in the subsequent layer, 
forming a fully connected structure. This simplicity allows FFNNs to be highly 
versatile, capable of modeling a wide variety of functions provided they have 
sufficient size and depth.

The Universal Approximation Theorem is a foundational result associated with 
FFNNs. It states that a feedforward network with at least one hidden layer, 
given a non-linear activation function and a sufficient number of neurons, can 
approximate any continuous function on compact subsets of real numbers to an 
arbitrary degree of accuracy. This theoretical guarantee means that, in 
principle, FFNNs are capable of solving virtually any learning problem. However, 
in practice, the ability to learn efficiently, generalize to new data, and train 
stably often demands more sophisticated architectures.

\subsection{Mathematical Formulation}

Each neuron within an FFNN performs a simple operation. It computes a weighted 
sum of its inputs, adds a bias term, and passes the result through an activation 
function to introduce non-linearity:

\begin{equation}
z = \sum_{i} w_i x_i + b
\end{equation}

\begin{equation}
a = \phi(z)
\end{equation}

Here, $x_i$ represents the inputs to the neuron, $w_i$ the learnable weights, 
$b$ the bias term, and $\phi$ the activation function, such as ReLU (Rectified 
Linear Unit), sigmoid, or tanh.

During training, the weights and biases are iteratively adjusted to minimize a 
loss function that quantifies the error between the network's predictions and 
the actual targets. This optimization is typically performed using variants of 
gradient descent and the backpropagation algorithm to compute gradients 
efficiently.

\subsection{Applications and Limitations}

Feedforward Neural Networks are widely used in tasks where the input data can be 
represented as fixed-size feature vectors. Examples include classification 
tasks, such as distinguishing between spam and non-spam emails, and regression 
tasks, such as predicting house prices based on property features.

Despite their theoretical flexibility, FFNNs encounter difficulties when dealing 
with data that has internal structure, such as images, sequences, or graphs. In 
such cases, treating all input features as independent can lead to 
inefficiencies and poor generalization. This limitation has motivated the 
development of more specialized architectures designed to exploit specific data 
structures.

A classic demonstration of FFNN capabilities is training on the MNIST dataset of 
handwritten digits. Even a relatively shallow network can achieve high accuracy, 
illustrating the power of simple architectures when applied appropriately.

\section{Convolutional Neural Networks (CNNs)}

\subsection{Motivation and Concept}

While FFNNs can process flat feature vectors effectively, they become 
inefficient when applied to structured spatial data, such as images. Images 
possess inherent local correlations: neighboring pixels tend to be related, 
forming edges, textures, and eventually higher-order shapes. Ignoring this 
structure, as FFNNs do, leads to models with unnecessary parameters, prone to 
overfitting and requiring vast amounts of training data.

Convolutional Neural Networks (CNNs) were developed to address these issues by 
embedding strong inductive biases into the architecture — assumptions about the 
nature of the input data — that make learning from spatial information far more 
efficient.

CNNs exploit two key ideas: local connectivity and weight sharing. Local 
connectivity means that each neuron in a convolutional layer connects only to a 
small, localized region of the previous layer, rather than to every neuron. 
Weight sharing means that the same set of weights (known as a filter or kernel) 
is applied across different parts of the input, dramatically reducing the number 
of learnable parameters.

\subsection{Structure and Operations}

The central operation in CNNs is the convolution. A small matrix (the filter) is 
slid across the input data, at each position computing a weighted sum that 
produces an activation. The result is a feature map that highlights regions 
where the learned feature is present.

Mathematically, the convolution operation can be expressed as:

\begin{equation}
(S \ast K)(i,j) = \sum_{m} \sum_{n} S(m,n)K(i - m,j - n)
\end{equation}

where $S$ is the input signal (e.g., an image), $K$ is the kernel, and $(i,j)$ 
denotes spatial coordinates.

In addition to convolutional layers, CNNs often include pooling layers that 
reduce the spatial dimensions of feature maps. Common pooling operations include 
max pooling (taking the maximum value within a window) and average pooling 
(taking the average), both of which promote translational invariance — the idea 
that features are recognized regardless of their precise location.

\subsection{Evolution of CNNs}

Early CNN architectures, such as LeNet-5 (1998), demonstrated the power of 
convolutions for tasks like handwritten digit recognition. However, it was 
AlexNet (2012) that propelled CNNs into prominence by winning the ImageNet 
challenge by a large margin, using deeper networks, ReLU activations, and GPU 
training.

Subsequent architectures like ResNet (2015) introduced innovations such as skip 
connections, allowing very deep networks (over 100 layers) to be trained 
effectively by mitigating vanishing gradient problems.

\subsection{Applications and Strengths}

CNNs have become the de facto standard for tasks involving spatial or grid-like 
data, including image classification, object detection, video analysis, and even
non-visual tasks like audio spectrogram processing. Their ability to efficiently 
learn spatial hierarchies of features makes them uniquely suited to these 
domains.

However, CNNs are less suited to tasks where sequential dependencies dominate, 
such as language processing, where the order of inputs carries critical meaning.

\section{Recurrent Neural Networks (RNNs)}

\subsection{Motivation and Concept}

Many types of data encountered in real-world applications are inherently 
sequential. Words in a sentence follow grammatical and semantic order. Stock 
prices unfold over time, influenced by prior values. Audio signals are sequences
 of pressure fluctuations. Standard FFNNs and CNNs, which process fixed-size 
 inputs independently, are poorly equipped to handle such temporal dependencies.

Recurrent Neural Networks (RNNs) introduce a form of memory into neural networks 
by allowing information to persist across time steps. At each step, an RNN 
processes one element of the sequence and updates a hidden state that captures 
information about past elements.

This recurrent structure allows RNNs to model dependencies of arbitrary length 
within sequences, making them suitable for tasks like language modeling, speech 
recognition, and time-series forecasting.

\subsection{Mathematical Description}

At each time step $t$, an RNN updates its hidden state $h_t$ according to the 
current input $x_t$ and the previous hidden state $h_{t-1}$:

\begin{equation}
h_t = \phi(W_{hh}h_{t-1} + W_{xh}x_t + b)
\end{equation}

where $W_{hh}$ and $W_{xh}$ are learned weight matrices, $b$ is a bias vector, 
and $\phi$ is a non-linear activation function such as tanh or ReLU.

The hidden state $h_t$ can be interpreted as a summary of all past inputs up to 
time $t$, providing the network with memory.

\subsection{Challenges and Solutions}

Despite their elegant design, vanilla RNNs suffer from significant limitations. 
During training, the gradients used for learning can either vanish or explode as 
they are propagated back through many time steps — phenomena known as the 
vanishing gradient and exploding gradient problems. As a result, RNNs often 
struggle to learn long-term dependencies.

To address these issues, specialized architectures such as Long Short-Term 
Memory (LSTM) networks and Gated Recurrent Units (GRUs) were developed. These
models introduce gates that control the flow of information, allowing the 
network to retain relevant memories over longer time spans while discarding 
irrelevant ones.

LSTMs and GRUs have become standard tools for sequence modeling, dramatically 
improving performance in tasks like machine translation, text generation, and 
automatic speech recognition.

\section{Transformer Networks — The New Standard}

\subsection{Motivation and Revolution}

While RNNs introduced memory into networks, they suffer from an inherent 
limitation: sequential processing. Each step depends on the previous one, making
it difficult to parallelize training and limiting the effective length of 
dependencies.

Transformer networks, introduced in the seminal paper 
"Attention Is All You Need" (2017), revolutionized sequence modeling by 
replacing recurrence with self-attention mechanisms. This shift enabled models 
to process entire sequences simultaneously, capturing long-range dependencies 
efficiently and leveraging parallel computation.

\subsection{Self-Attention Mechanism}

The core idea of the Transformer is self-attention, where each input element 
attends to every other element in the sequence, learning relationships 
regardless of their distance.

Given an input sequence represented by embeddings $X$, three matrices are 
computed through learned linear transformations:

\begin{itemize}
    \item Query (Q)
    \item Key (K)
    \item Value (V)
\end{itemize}

The attention scores are calculated as:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

where $d_k$ is the dimensionality of the keys, and softmax normalizes the 
attention scores.

Multiple attention heads are used in parallel (multi-head attention), 
allowing the model to capture different types of relationships simultaneously. 
Positional encoding is added to the input embeddings to preserve the order of 
tokens, since the self-attention mechanism itself is order-agnostic.

\subsection{Architectures and Dominance}

Transformers come in two primary forms:

\begin{itemize}
    \item Encoder-decoder models (e.g., original Transformer for translation 
    tasks)
    \item Decoder-only models (e.g., GPT series for language generation)
\end{itemize}

Today, Transformer-based models dominate fields ranging from Natural Language 
Processing (BERT, GPT, T5) to Computer Vision (Vision Transformer, ViT) and 
multimodal learning (CLIP, Flamingo).

Their ability to scale to billions of parameters and train on massive datasets 
has driven much of the current wave of AI innovation.

\section{Generative Adversarial Networks (GANs)}

\subsection{Concept and Dynamic}

Generative Adversarial Networks (GANs), introduced by Ian Goodfellow in 2014, 
represent a fundamentally different approach to learning. Rather than optimizing 
a single model, GANs consist of two models in competition: a generator that 
tries to produce realistic synthetic data, and a discriminator that tries to 
distinguish between real and synthetic data.

During training, the generator improves by learning to fool the discriminator, 
while the discriminator improves by learning to better detect fakes. This 
adversarial dynamic drives both models to become increasingly sophisticated.

\subsection{Challenges and Impact}

Training GANs is notoriously delicate. If the discriminator becomes too 
powerful, it provides no meaningful gradient for the generator. If the generator 
becomes too powerful early, the discriminator fails to learn. Various 
techniques, such as careful architecture design, progressive training, and 
Wasserstein distances, have been developed to stabilize GAN training.

GANs have achieved remarkable success in areas such as synthetic media 
generation (DeepFakes), high-resolution image synthesis, art creation, and data 
augmentation for low-resource machine learning tasks.

\section{Chapter Summary}

Throughout this chapter, we have traced the evolution of neural network 
architectures from the simple feedforward models foundational to deep learning, 
to convolutional networks specialized for spatial data, recurrent networks 
designed for sequences, and transformers that now underpin the most advanced AI 
systems. We have also explored adversarial models that unleash the generative 
creativity of AI through competition.

Each architectural innovation reflects a deeper understanding of the structure 
of real-world data and the challenges of efficient learning. As we move forward, 
understanding these architectures will provide a crucial foundation for engaging 
with cutting-edge developments in Artificial Intelligence.

\end{document}